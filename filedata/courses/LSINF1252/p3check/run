#!/bin/python3

# Check archive
import check_archive

# Imports
import subprocess, shlex, re, os, glob
from xml.etree import ElementTree
from inginious import feedback, rst, input

# Switch working directory to student/
os.chdir("student")

# Parse tests annotations
annotations = []
files = glob.glob("*.c")
for f in files:
    test = re.findall("@(.*):(.*):(.*) => \[(.*)]", open(f).read())
    annotations.extend(test)   
    
# Compilation
p = subprocess.Popen(shlex.split("make"), stderr=subprocess.STDOUT, stdout=subprocess.PIPE)
make_output = p.communicate()[0].decode('utf-8')
# If compilation failed, exit with "failed" result
if p.returncode:
    feedback.set_global_result("failed")
    feedback.set_global_feedback("La compilation de votre code a échoué. Voici le message de sortie de la commande ``make`` :")
    feedback.set_global_feedback(rst.get_codeblock('', make_output), True)
    exit(0)
else:
    feedback.set_global_result("success")
    feedback.set_global_feedback("- Votre code compile.\n")

# Remove source files
subprocess.run("rm -rf *.c *.tpl *.h *.o", shell=True)

# Run the code in a parallel container
p = subprocess.Popen(shlex.split("run_student --time 20 --hard-time 60 ./tests"), stderr=subprocess.STDOUT, stdout=subprocess.PIPE)
p.communicate() #run_output = p.communicate()[0].decode('utf-8')
# If run failed, exit with "failed" result
if p.returncode:
    feedback.set_global_result("failed")
    if p.returncode == 256-8:
        montest_output = rst.get_admonition("warning", "**Erreur d'exécution**", "Votre code a produit une erreur. Le signal SIGFPE a été envoyé : *Floating Point Exception*.")
    elif p.returncode == 256-11:
       montest_output = rst.get_admonition("warning", "**Erreur d'exécution**", "Votre code a produit une erreur. Le signal SIGSEGV a été envoyé : *Segmentation Fault*.")
    elif p.returncode == 252:
        montest_output = rst.get_admonition("warning", "**Erreur d'exécution**", "Votre code a tenté d'allouer plus de mémoire que disponible.")
    elif p.returncode == 253:
        montest_output = rst.get_admonition("warning", "**Erreur d'exécution**", "Votre code a pris trop de temps pour s'exécuter.")
    else:
        montest_output = rst.get_admonition("warning", "**Erreur d'exécution**", "Votre code a produit une erreur.")
    feedback.set_global_feedback(rst.indent_block(2, montest_output, " "), True)    
    exit(0)
#elif run_output:   
#    feedback.set_global_feedback("- Sortie de votre méthode de test:\n" + rst.indent_block(2, rst.get_codeblock('', run_output), " "), True)

# Comment to run the tests
#feedback.set_global_feedback("- **Cette note n'est pas finale.** Une série de tests sera exécutée sur votre code après l'examen.\n", True)
#exit(0)

# Fetch CUnit test results
tree = ElementTree.parse('CUnitAutomated-Results.xml') 
tests_failed = set([item.find('TEST_NAME').text.strip() for item in tree.getroot().iter('CUNIT_RUN_TEST_FAILURE')])
tests_succeeded = set([item.find('TEST_NAME').text.strip() for item in tree.getroot().iter('CUNIT_RUN_TEST_SUCCESS')])

# Produce feedback
if len(tests_failed) == 0:
    feedback.set_global_feedback("\n- Votre code a passé tous les tests.", True)
else:
    feedback.set_global_feedback("\n- Il y a des erreurs dans votre solution.", True)
    
feedback.set_global_feedback("\n\n**Cette note est partielle et ne prend pas en compte les fonctions** ``save`` **et** ``load`` **. Une note additionnelle est attribuée pour ces fonctions ainsi que pour le rapport fourni.**", True)

# Parse tests description
tests_descr = {a[1]:{'pid':a[0], 'weight': int(a[2]), 'desc':a[3]} for a in annotations}

# Succeeded tests
score = 0
for success in tests_succeeded:
    pid = tests_descr[success]['pid']
    score += tests_descr[success]['weight']
    feedback.set_problem_result("success", pid)
    feedback.set_problem_feedback("* " + tests_descr[success]['desc']+"\n\n  => réussi (" + str(tests_descr[success]['weight']) + "/" + str(tests_descr[success]['weight']) + " pts)\n\n", pid, True)

# Failed tests
total = score
for fail in tests_failed:
    pid = tests_descr[fail]['pid']
    total += tests_descr[fail]['weight']
    feedback.set_problem_result("failed", pid)
    feedback.set_problem_feedback("* " + tests_descr[fail]['desc']+"\n\n  => échoué (0/" + str(tests_descr[fail]['weight']) + " pts)\n\n", pid, True)

score = 100*score/(total if not total == 0 else 1)
feedback.set_grade(score)
feedback.set_global_result("success" if score >= 50 else "failed")

